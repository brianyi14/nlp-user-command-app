{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_data = pd.read_csv('./data/augmented_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes(object):\n",
    "    #initialize model\n",
    "    def __init__(self,data,training_split,laplace):\n",
    "        self.num_rows = len(data)\n",
    "        #shuffle data and reset indexes\n",
    "        self.shuffled_data = data.sample(frac=1).reset_index(drop=True)\n",
    "        #split the data into training and testing sets using input proportion \n",
    "        (self.training_data,self.testing_data) = self.training_testing_split(self.shuffled_data,training_split)\n",
    "        self.num_training_rows,self.num_testing_rows = len(self.training_data),len(self.testing_data)\n",
    "        self.smoothing_constant = laplace\n",
    "        \n",
    "    def training_testing_split(self,all_data,training_split):\n",
    "        breakoff = int(self.num_rows*training_split)\n",
    "        return (all_data.loc[:breakoff],all_data[['Text Command','Action']].loc[breakoff:])\n",
    "         \n",
    "    def action_map(self):\n",
    "        return {0: 'To Do',1: 'In Progress',2: 'In Review',3: 'Blocked',4: 'Completed'}\n",
    "    \n",
    "    def index_action(self):\n",
    "        return {'To Do': 0,'In Progress': 1,'In Review': 2,'Blocked': 3,'Completed': 4}\n",
    "    #words to ignore when calculating probabilities\n",
    "    def stopwords(self):\n",
    "        return [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
    "\n",
    "    def compute_probabilities(self,text_command_training_data):\n",
    "        action_map = self.action_map()\n",
    "        action_probabilities = dict()\n",
    "        action_dict = dict()\n",
    "        word_dict = dict()\n",
    "        word_counter = 0\n",
    "        stopwords = self.stopwords()\n",
    "        for action in list(action_map.values()):\n",
    "            action_dict[action] = dict()\n",
    "        for i in list(text_command_training_data.index.values):\n",
    "            action = text_command_training_data.loc[i][1]\n",
    "            action_probabilities[action] = action_probabilities.get(action,0) + 1\n",
    "            text_command = text_command_training_data.loc[i][0]\n",
    "            for word in text_command.lower().split():\n",
    "                if word not in stopwords:\n",
    "                    word_counter += 1\n",
    "                    word_dict[word] = word_dict.get(word,0) + 1\n",
    "                    action_dict[action][word] = action_dict[action].get(word,0) + 1\n",
    "        num_unique_words = len(word_dict)\n",
    "        for action in action_dict:\n",
    "            num_words = sum(list(action_dict[action].values()))\n",
    "            for word in action_dict[action]:\n",
    "                action_dict[action][word] = (action_dict[action][word]+self.smoothing_constant)/(num_words+self.smoothing_constant*num_unique_words)\n",
    "        for action in action_probabilities:\n",
    "            action_probabilities[action] = action_probabilities[action]/self.num_training_rows\n",
    "        \n",
    "        for word in word_dict:\n",
    "            word_dict[word] = (word_dict[word]+self.smoothing_constant)/(word_counter+self.smoothing_constant*num_unique_words)\n",
    "        self.word_counter = word_counter\n",
    "        return (action_probabilities,action_dict,word_dict)\n",
    "    \n",
    "    def train(self):\n",
    "        text_command_and_action = self.training_data[['Text Command','Action']]\n",
    "        (self.action_probabilities,self.action_dict,self.word_dict) = self.compute_probabilities(text_command_and_action)\n",
    "    \n",
    "    def predict(self,data):\n",
    "        num_unique_words = len(self.word_dict)\n",
    "        action_map = self.action_map()\n",
    "        predictions = pd.DataFrame(columns=['Text Command', 'Predicted Action','Predicted Probabilities'])\n",
    "        stopwords = self.stopwords()\n",
    "        for i in list(data.index.values):\n",
    "            words = data.loc[i][0].lower().split()\n",
    "            action_probabilities = []\n",
    "            for action in list(action_map.values()):\n",
    "                num_words = sum(list(self.action_dict[action].values()))\n",
    "                action_probability = self.action_probabilities[action]\n",
    "                probability = 1\n",
    "                denominator = 1\n",
    "                for word in words:\n",
    "                    if word not in stopwords:\n",
    "                        probability = probability * self.action_dict[action].get(word,(self.smoothing_constant)/(num_words+self.smoothing_constant*num_unique_words))\n",
    "                        denominator = denominator * self.word_dict.get(word,(self.smoothing_constant)/(self.word_counter+self.smoothing_constant*num_unique_words))\n",
    "                probability = (probability * action_probability)/denominator\n",
    "    \n",
    "                \n",
    "                action_probabilities.append(probability)\n",
    "            action_probabilities = self.softmax(action_probabilities)\n",
    "            \n",
    "            predictions.loc[i]= [data.loc[i][0],action_map[action_probabilities.index(max(action_probabilities))],action_probabilities]\n",
    "        return predictions\n",
    "    def test(self):\n",
    "        action_map = self.action_map()\n",
    "        correct = 0\n",
    "        preds = self.predict(self.testing_data)\n",
    "        actions = self.testing_data['Action']\n",
    "        for i in list(self.testing_data.index.values):\n",
    "            if actions.loc[i][0] == preds['Predicted Action'].loc[i][0]:\n",
    "                correct += 1\n",
    "        losses = self.cross_entropy_loss(pd.concat([actions,preds['Predicted Probabilities']],axis=1))\n",
    "                \n",
    "        print('Accuracy: ' + str(correct/self.num_testing_rows))\n",
    "        #for loss_idx in losses:\n",
    "            #print('Cross Entropy Loss '+action_map[loss_idx]+': '+str(losses[loss_idx]))\n",
    "        preds['Predicted Probabilities']\n",
    "        \n",
    "    def softmax(self,labels):\n",
    "        softmax_labels = []\n",
    "        denominator = sum([math.exp(label) for label in labels])\n",
    "        for label in labels:\n",
    "            softmax_labels.append(math.exp(label)/denominator)\n",
    "        return softmax_labels\n",
    "    \n",
    "    def cross_entropy_loss(self,labels):\n",
    "        index_action = self.index_action()\n",
    "        losses = {0: [],1: [],2: [],3: [], 4: []}\n",
    "        for i in list(labels.index.values):\n",
    "            idx = index_action[labels.loc[i][0]]\n",
    "            loss = - math.log(labels.loc[i][1][idx])\n",
    "            losses[idx].append(loss)\n",
    "        for label in losses:\n",
    "            losses[label] = sum(losses[label])/len(losses[label])\n",
    "        return losses\n",
    "            \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = NaiveBayes(augmented_data,0.8,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.974025974025974\n"
     ]
    }
   ],
   "source": [
    "classifier.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
