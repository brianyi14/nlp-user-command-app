{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import math\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset \n",
    "augmented_data = pd.read_csv('./data/Augmented_Data2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text Command</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Identifier</th>\n",
       "      <th>Action</th>\n",
       "      <th>One Hot Encoded Topic</th>\n",
       "      <th>One Hot Encoded Action</th>\n",
       "      <th>Verb/Noun</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>finish plan Developing editorial calendar for ...</td>\n",
       "      <td>Project</td>\n",
       "      <td>Developing editorial calendar for content sharing</td>\n",
       "      <td>Completed</td>\n",
       "      <td>[0,1]</td>\n",
       "      <td>[0,0,0,0,1]</td>\n",
       "      <td>SYNREPLACED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>move plan Developing editorial calendar for co...</td>\n",
       "      <td>Project</td>\n",
       "      <td>Developing editorial calendar for content sharing</td>\n",
       "      <td>At Risk</td>\n",
       "      <td>[0,1]</td>\n",
       "      <td>[0,0,1,0,0]</td>\n",
       "      <td>SYNREPLACED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fixed Developing editorial calendar for conten...</td>\n",
       "      <td>Task</td>\n",
       "      <td>Developing editorial calendar for content sharing</td>\n",
       "      <td>In Review</td>\n",
       "      <td>[1,0]</td>\n",
       "      <td>[0,0,1,0,0]</td>\n",
       "      <td>SYNREPLACED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>audit task Developing editorial calendar for c...</td>\n",
       "      <td>Task</td>\n",
       "      <td>Developing editorial calendar for content sharing</td>\n",
       "      <td>In Review</td>\n",
       "      <td>[1,0]</td>\n",
       "      <td>[0,0,1,0,0]</td>\n",
       "      <td>SYNREPLACED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>offset on Developing editorial calendar for co...</td>\n",
       "      <td>Task</td>\n",
       "      <td>Developing editorial calendar for content sharing</td>\n",
       "      <td>In Progress</td>\n",
       "      <td>[1,0]</td>\n",
       "      <td>[0,1,0,0,0]</td>\n",
       "      <td>SYNREPLACED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6974</th>\n",
       "      <td>end throw Reviewing website backlinks</td>\n",
       "      <td>Project</td>\n",
       "      <td>Reviewing website backlinks</td>\n",
       "      <td>Completed</td>\n",
       "      <td>[0,1]</td>\n",
       "      <td>[0,0,0,0,1]</td>\n",
       "      <td>SYNREPLACED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6975</th>\n",
       "      <td>to do Reviewing website backlinks plan</td>\n",
       "      <td>Project</td>\n",
       "      <td>Reviewing website backlinks</td>\n",
       "      <td>Create</td>\n",
       "      <td>[0,1]</td>\n",
       "      <td>[1,0,0,0,0]</td>\n",
       "      <td>SYNREPLACED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6976</th>\n",
       "      <td>Reviewing website backlinks project</td>\n",
       "      <td>Task</td>\n",
       "      <td>Reviewing website backlinks</td>\n",
       "      <td>To Do</td>\n",
       "      <td>[1,0]</td>\n",
       "      <td>[1,0,0,0,0]</td>\n",
       "      <td>SYNREPLACED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6977</th>\n",
       "      <td>Reviewing website backlinks chore was complete</td>\n",
       "      <td>Task</td>\n",
       "      <td>Reviewing website backlinks</td>\n",
       "      <td>Completed</td>\n",
       "      <td>[1,0]</td>\n",
       "      <td>[0,0,0,0,1]</td>\n",
       "      <td>SYNREPLACED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6978</th>\n",
       "      <td>through with task Reviewing website backlinks</td>\n",
       "      <td>Task</td>\n",
       "      <td>Reviewing website backlinks</td>\n",
       "      <td>Completed</td>\n",
       "      <td>[1,0]</td>\n",
       "      <td>[0,0,0,0,1]</td>\n",
       "      <td>SYNREPLACED</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6979 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Text Command    Topic  \\\n",
       "0     finish plan Developing editorial calendar for ...  Project   \n",
       "1     move plan Developing editorial calendar for co...  Project   \n",
       "2     fixed Developing editorial calendar for conten...     Task   \n",
       "3     audit task Developing editorial calendar for c...     Task   \n",
       "4     offset on Developing editorial calendar for co...     Task   \n",
       "...                                                 ...      ...   \n",
       "6974              end throw Reviewing website backlinks  Project   \n",
       "6975             to do Reviewing website backlinks plan  Project   \n",
       "6976                Reviewing website backlinks project     Task   \n",
       "6977     Reviewing website backlinks chore was complete     Task   \n",
       "6978      through with task Reviewing website backlinks     Task   \n",
       "\n",
       "                                             Identifier       Action  \\\n",
       "0     Developing editorial calendar for content sharing    Completed   \n",
       "1     Developing editorial calendar for content sharing      At Risk   \n",
       "2     Developing editorial calendar for content sharing    In Review   \n",
       "3     Developing editorial calendar for content sharing    In Review   \n",
       "4     Developing editorial calendar for content sharing  In Progress   \n",
       "...                                                 ...          ...   \n",
       "6974                        Reviewing website backlinks    Completed   \n",
       "6975                        Reviewing website backlinks       Create   \n",
       "6976                        Reviewing website backlinks        To Do   \n",
       "6977                        Reviewing website backlinks    Completed   \n",
       "6978                        Reviewing website backlinks    Completed   \n",
       "\n",
       "     One Hot Encoded Topic One Hot Encoded Action    Verb/Noun  \n",
       "0                    [0,1]            [0,0,0,0,1]  SYNREPLACED  \n",
       "1                    [0,1]            [0,0,1,0,0]  SYNREPLACED  \n",
       "2                    [1,0]            [0,0,1,0,0]  SYNREPLACED  \n",
       "3                    [1,0]            [0,0,1,0,0]  SYNREPLACED  \n",
       "4                    [1,0]            [0,1,0,0,0]  SYNREPLACED  \n",
       "...                    ...                    ...          ...  \n",
       "6974                 [0,1]            [0,0,0,0,1]  SYNREPLACED  \n",
       "6975                 [0,1]            [1,0,0,0,0]  SYNREPLACED  \n",
       "6976                 [1,0]            [1,0,0,0,0]  SYNREPLACED  \n",
       "6977                 [1,0]            [0,0,0,0,1]  SYNREPLACED  \n",
       "6978                 [1,0]            [0,0,0,0,1]  SYNREPLACED  \n",
       "\n",
       "[6979 rows x 7 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive Bayes classifier\n",
    "class NaiveBayes(object):\n",
    "    #initialize model\n",
    "    def __init__(self,data,training_split,use_case,laplace):\n",
    "        '''\n",
    "        use_case: what is being predicted by the Naive Bayes classifier\n",
    "        data: dataset that is being trained on\n",
    "        training_split: percent of data that should be trained on expressed as a decimal\n",
    "        laplace: smoothing constant for laplace smoothing\n",
    "        '''\n",
    "        self.use_case = use_case\n",
    "        #depending on the use case of the model, select the appropriate data that will be trained on\n",
    "        if self.use_case == 'topic':\n",
    "            self.columns = ['Text Command', 'Topic']\n",
    "        else:\n",
    "            self.columns = ['Text Command','Action']\n",
    "            #depending on whether the model is predicting task or project actions, select the appropriate data\n",
    "            if self.use_case == 'task action':\n",
    "                data = data[data['Topic'] == 'Task']\n",
    "            else:\n",
    "                data = data[data['Topic'] == 'Project']\n",
    "        self.num_rows = len(data)\n",
    "        #shuffle data and reset indexes\n",
    "        self.shuffled_data = data.sample(frac=1).reset_index(drop=True)\n",
    "        #split the data into training and testing sets using inputted training split parameter \n",
    "        (self.training_data,self.testing_data) = self.training_testing_split(self.shuffled_data,training_split)\n",
    "        self.num_training_rows,self.num_testing_rows = len(self.training_data),len(self.testing_data)\n",
    "        self.smoothing_constant = laplace\n",
    "        #depending on the use case of the model, create a dictionary object that maps labels to indexes\n",
    "        if self.use_case == 'topic':\n",
    "            index_action = {'Task':0, 'Project':1}\n",
    "        if self.use_case == 'task action':\n",
    "            index_action = {'To Do': 0, 'In Progress':1, 'In Review':2, 'Blocked':3, 'Completed':4}\n",
    "        if self.use_case == 'project action':\n",
    "            index_action = {'Create':0, 'On Target':1, 'At Risk':2, 'Danger':3, 'Completed':4}\n",
    "        self.index_action = index_action\n",
    "        #create another dictionary object that reverses the index_action dictionary and maps indexes to labels\n",
    "        self.label_map = dict()\n",
    "        for key in self.index_action:\n",
    "            self.label_map[self.index_action[key]] = key\n",
    "            \n",
    "    #function for returning training and testing data based on inputted data and percentage split \n",
    "    def training_testing_split(self,all_data,training_split):\n",
    "        breakoff = int(self.num_rows*training_split)\n",
    "        return (all_data.loc[:breakoff],all_data[self.columns].loc[breakoff:])\n",
    "    \n",
    "    #words to ignore when calculating probabilities\n",
    "    def stopwords(self):\n",
    "        return [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
    "    \n",
    "    #function for calculating probabilities that will be used for making predictions on text commands\n",
    "    def compute_probabilities(self,text_command_training_data):\n",
    "        action_map = self.label_map\n",
    "        #dictionary for updating probabilities of a certain label occuring\n",
    "        action_probabilities = dict()\n",
    "        #nested dictionary for storing the probabilities of words occuring in text commands with a specific label\n",
    "        action_dict = dict()\n",
    "        #dictionary for stroing probability of a word occuring \n",
    "        word_dict = dict()\n",
    "        word_counter = 0\n",
    "        stopwords = self.stopwords()\n",
    "        #iterate over possible labels for given use case\n",
    "        for action in list(action_map.values()):\n",
    "            #create a dictionary for every possible label that contains probabilities of words \n",
    "            action_dict[action] = dict()\n",
    "        #iterate over index values in training dataset\n",
    "        for i in list(text_command_training_data.index.values):\n",
    "            #pick out true label from training dataset \n",
    "            action = text_command_training_data.loc[i][1]\n",
    "            #update the count of a label occuring in the dataset\n",
    "            action_probabilities[action] = action_probabilities.get(action,0) + 1\n",
    "            #pick out the text command from the training dataset\n",
    "            text_command = text_command_training_data.loc[i][0]\n",
    "            #iterate over the words in the text command\n",
    "            for word in text_command.lower().split():\n",
    "                #if the words is not a stopword\n",
    "                if word not in stopwords:\n",
    "                    #update the total number of words occuring in the training dataset\n",
    "                    word_counter += 1\n",
    "                    #update the count of a specific word \n",
    "                    word_dict[word] = word_dict.get(word,0) + 1\n",
    "                    #update the count of a specific word occuring in a text command with a specific label\n",
    "                    action_dict[action][word] = action_dict[action].get(word,0) + 1\n",
    "        #number of unique words that have been seen in the training dataset\n",
    "        num_unique_words = len(word_dict)\n",
    "        #iterate over possible labels \n",
    "        for action in action_dict:\n",
    "            #total number of words that occur in text commands with a specific label \n",
    "            num_words = sum(list(action_dict[action].values()))\n",
    "            #iterate over the word counts for specific words in text command with a specific label\n",
    "            for word in action_dict[action]:\n",
    "                '''\n",
    "                probability for a word occuring in a text command with a specific label is given by:\n",
    "                (number of times word occurs in text commands with a specific label + smoothing constant) / (total number of words in text commands with a specific label + smoothing constant * number of unique words in text commands across entire dataset)\n",
    "                '''\n",
    "                action_dict[action][word] = (action_dict[action][word]+self.smoothing_constant)/(num_words+self.smoothing_constant*num_unique_words)\n",
    "        #iterate over possible labels\n",
    "        for action in action_probabilities:\n",
    "            #convert the count of specific label into probability by dividing count by number of rows in training dataset\n",
    "            action_probabilities[action] = action_probabilities[action]/self.num_training_rows\n",
    "        #iterate over words that occur in the training dataset\n",
    "        for word in word_dict:\n",
    "            #convert the count of a specific word into probability using same formula\n",
    "            word_dict[word] = (word_dict[word]+self.smoothing_constant)/(word_counter+self.smoothing_constant*num_unique_words)\n",
    "        self.word_counter = word_counter\n",
    "        return (action_probabilities,action_dict,word_dict)\n",
    "    \n",
    "    def train(self):\n",
    "        #select the appropriate data to calculate probabilities \n",
    "        text_command_and_action = self.training_data[self.columns]\n",
    "        #compute and return probabilties needed to make label predictions\n",
    "        (self.action_probabilities,self.action_dict,self.word_dict) = self.compute_probabilities(text_command_and_action)\n",
    "    \n",
    "    \n",
    "    #function for making label prediction on inputted dataset with text commands and labels\n",
    "    '''\n",
    "    predict function below uses Bayes Theorem: P(A|B) = P(A and B)/P(B) = (P(B|A) * P(A))/P(B)\n",
    "    using this theorem for the purposes of making label predictions on text commands: \n",
    "    P(label|text command) = (P(text command|label)*P(label))/P(text command)\n",
    "    where P(text command|label) = P(1st word in text command|label)*P(2nd word in text command|label)*...\n",
    "    and P(text command) = P(text command|label1)*P(label1) + P(text command|label2)*P(label2) + ...\n",
    "    \n",
    "    '''\n",
    "    def predict(self,data):\n",
    "        num_unique_words = len(self.word_dict)\n",
    "        action_map = self.label_map\n",
    "        predictions = pd.DataFrame(columns=[self.columns[0], 'Predicted Action','Predicted Probabilities'])\n",
    "        stopwords = self.stopwords()\n",
    "        #iterate over inputted dataset\n",
    "        for i in list(data.index.values):\n",
    "            #pick out the words in the text command\n",
    "            words = data.loc[i][0].lower().split()\n",
    "            #list that will have probabilities assigned to each label\n",
    "            action_probabilities = []\n",
    "            #placeholder for denominator in Bayes Theorem\n",
    "            denominator = 0\n",
    "            #iterate over the values in the dictionary object mapping index to label\n",
    "            for action in list(action_map.values()):\n",
    "                #total number of words in text commands with specific label\n",
    "                num_words = sum(list(self.action_dict[action].values()))\n",
    "                #placeholder for term in denominator in Bayes Theorem\n",
    "                probability = 1\n",
    "                #iterate over words in text command\n",
    "                for word in words:\n",
    "                    if word not in stopwords:\n",
    "                        #updates numerator probability by multiplying numerator by P(word|label) which is calculated previously\n",
    "                        probability = probability * self.action_dict[action].get(word,(self.smoothing_constant)/(num_words+self.smoothing_constant*num_unique_words))\n",
    "                #update denominator probability by adding P(text command|label)*P(label)\n",
    "                denominator += probability*self.action_probabilities[action]\n",
    "            #iterate over values in dictionary object mapping index to label\n",
    "            for action in list(action_map.values()):\n",
    "                #total number of words in text commands with specific label\n",
    "                num_words = sum(list(self.action_dict[action].values()))\n",
    "                #probability of specific label occuring\n",
    "                action_probability = self.action_probabilities[action]\n",
    "                #placeholder for numerator in Bayes Theorem\n",
    "                probability = 1\n",
    "                #iterate over words in text command\n",
    "                for word in words:\n",
    "                    if word not in stopwords:\n",
    "                        #update numerator probability by multiplying numerator by P(word|label)\n",
    "                        probability = probability * self.action_dict[action].get(word,(self.smoothing_constant)/(num_words+self.smoothing_constant*num_unique_words))\n",
    "                #probability of label given text command    \n",
    "                probability = (probability * action_probability)/denominator\n",
    "    \n",
    "                #add probability of text command being specific label to list\n",
    "                action_probabilities.append(probability)\n",
    "            #add prediction to prediction dataset by finding the index of the maximum probability and then use that index in the index to label dictionary to find \n",
    "            #predicted label\n",
    "            predictions.loc[i]= [data.loc[i][0],action_map[action_probabilities.index(max(action_probabilities))],action_probabilities]\n",
    "        return predictions\n",
    "    \n",
    "    #test trained probabilites on testing dataset\n",
    "    def test(self):\n",
    "        action_map = self.label_map\n",
    "        count = dict()\n",
    "        #track how many predictions are correct\n",
    "        correct = 0\n",
    "        #dataset with predictions from testing data\n",
    "        preds = self.predict(self.testing_data)\n",
    "        #ground truth labels in the testing dataset\n",
    "        actions = self.testing_data[self.columns[1]]\n",
    "        #iterate over rows in testing dataset\n",
    "        for i in list(self.testing_data.index.values):\n",
    "            #check if predicted label equals ground truth label\n",
    "            if actions.loc[i][0] == preds['Predicted Action'].loc[i][0]:\n",
    "                correct += 1\n",
    "        #compute the average NLL loss for each label\n",
    "        losses = self.cross_entropy_loss(pd.concat([actions,preds['Predicted Probabilities']],axis=1))\n",
    "        #calculate accuracy of testing predictions\n",
    "        accuracy = correct/self.num_testing_rows\n",
    "        return accuracy, losses, preds\n",
    "    \n",
    "    #function for computing average negative log likelihood loss for each label\n",
    "    def cross_entropy_loss(self,labels):\n",
    "        index_action = self.index_action\n",
    "        losses = dict()\n",
    "        for key in self.label_map:\n",
    "            losses[key] = []\n",
    "        for i in list(labels.index.values):\n",
    "            idx = index_action[labels.loc[i][0]]\n",
    "            loss = - math.log(labels.loc[i][1][idx])\n",
    "            losses[idx].append(loss)\n",
    "        for label in losses:\n",
    "            losses[label] = sum(losses[label])/len(losses[label])\n",
    "        return losses\n",
    "            \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "Accuracy: 0.8982020057306589\n",
      "Cross Entropy Losses: [0.25483009624163233, 0.26600454722051753]\n"
     ]
    }
   ],
   "source": [
    "topic_accuracies = []\n",
    "cross_entropy_losses = [0,0]\n",
    "for i in range(100):\n",
    "    classifier = NaiveBayes(augmented_data,0.8,'topic',1)\n",
    "    classifier.train()\n",
    "    accuracy, losses, preds = classifier.test()\n",
    "    losses = list(losses.values())\n",
    "    counts = [0,0]\n",
    "    \n",
    "    topic_accuracies.append(accuracy)\n",
    "    cross_entropy_losses = [sum(x) for x in zip(cross_entropy_losses,losses)]\n",
    "    if i%10 == 0:\n",
    "        print(i)\n",
    "print('Accuracy: ' + str(sum(topic_accuracies)/len(topic_accuracies)))\n",
    "print('Cross Entropy Losses: ' + str([x/len(topic_accuracies) for x in cross_entropy_losses]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "0.5969129287598949\n",
      "[2.065536718281523, 1.2081753466600305, 1.1558338185991048, 1.5290459803963214, 1.2461902320459644]\n"
     ]
    }
   ],
   "source": [
    "topic_accuracies = []\n",
    "cross_entropy_losses = [0,0,0,0,0]\n",
    "for i in range(100):\n",
    "    classifier = NaiveBayes(augmented_data,0.8,'task action',1)\n",
    "    classifier.train()\n",
    "    accuracy, losses, preds = classifier.test()\n",
    "    losses = list(losses.values())\n",
    "    topic_accuracies.append(accuracy)\n",
    "    cross_entropy_losses = [sum(x) for x in zip(cross_entropy_losses,losses)]\n",
    "    if i%10 == 0:\n",
    "        print(i)\n",
    "print('Accuracy: ' + str(sum(topic_accuracies)/len(topic_accuracies)))\n",
    "print('Cross Entropy Losses: ' + str([x/len(topic_accuracies) for x in cross_entropy_losses]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "0.6427586206896551\n",
      "[1.8113355350334601, 0.9430479019411666, 1.067495212015915, 1.6394451040277886, 1.035731916757897]\n"
     ]
    }
   ],
   "source": [
    "topic_accuracies = []\n",
    "cross_entropy_losses = [0,0,0,0,0]\n",
    "for i in range(100):\n",
    "    classifier = NaiveBayes(augmented_data,0.8,'project action',1)\n",
    "    classifier.train()\n",
    "    accuracy, losses, preds = classifier.test()\n",
    "    losses = list(losses.values())\n",
    "    topic_accuracies.append(accuracy)\n",
    "    cross_entropy_losses = [sum(x) for x in zip(cross_entropy_losses,losses)]\n",
    "    if i%10 == 0:\n",
    "        print(i)\n",
    "print('Accuracy: ' + str(sum(topic_accuracies)/len(topic_accuracies)))\n",
    "print('Cross Entropy Losses: ' + str([x/len(topic_accuracies) for x in cross_entropy_losses]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
