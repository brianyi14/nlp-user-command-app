#clone the github repository with pytorch files 
git clone https://github.com/brianyi14/nlp-user-command-app

#change directory to access cloned files
cd nlp-user-command-app/torchserve

#create Google Cloud artifact repository which will contain models
gcloud beta artifacts repositories create getting-started-pytorch \
 --repository-format=docker \
 --location=us-east1

#Dockerfile which will run the torchserve model archiver. This file instructs Docker to run
#a container that packages the different files into a model archive file(.mar). It then runs
#a command which starts the torchserve server using the configuration file we have created
#and the model archive file that was just created
cat > Dockerfile <<END
FROM pytorch/torchserve:0.2.0-cpu

COPY project_action/project_action_lstm.py project_action/project_action_trained.pt project_action/project_action_handler.py configuration.properties requirements.txt /home/model-server/


RUN torch-model-archiver \
  --model-name=lstm \
  --version=1.0 \
  --model-file=/home/model-server/project_action_lstm.py \
  --serialized-file=/home/model-server/project_action_trained.pt \
  --handler=/home/model-server/project_action_handler.py \
  --export-path=/home/model-server/model-store \
  --requirements-file=/home/model-server/requirements.txt

CMD ["torchserve", \
     "--start", \ 
     "--ts-config=/home/model-server/configuration.properties", \
     "--models", \
     "lstm=lstm.mar"]
     

END

#build the docker container using the supplied Dockerfile
docker build \
  --tag=us-east1-docker.pkg.dev/user-command-nlp/getting-started-pytorch/serve-lstm \
  .

#start the local torchserve server on ports 8080 and 8081 using the built docker container. Port
#8080 handles client requests for model predictions. Port 8081 handles requests to change model
#server parameters 
docker run -d -p 8080:8080 -p 8081:8081 --name=local_lstm \
  us-east1-docker.pkg.dev/user-command-nlp/getting-started-pytorch/serve-lstm



#ping the server which tells you whether there is something wrong with the archived model.
#An unhealthy response means there is some error in one of the files supplied to the model 
#archiver. A healthy response means the files are correctly formatted. 
curl http://localhost:8080/ping

curl -X POST "http://localhost:8081/models?url=lstm.mar&initial_workers=1"

#set the minimum number of workers for the model so it can make predictions
curl -v -X PUT localhost:8081/models/lstm?min_worker=2
curl -v -X PUT http://localhost:8081/models/lstm?min_worker=2&synchronous=true
  
curl -d {"command":"Finished with task data cleaning"} -H "Content-Type: application/json" -X POST http://localhost:8080/predictions/lstm
curl --header "Content-Type: application/json" \
  --request POST \
  --data '{"command":"Finished with task data cleaning"}'  \
  localhost:8080/predictions/lstm_topic

docker push us-east1-docker.pkg.dev/user-command-nlp/getting-started-pytorch/serve-lstm


gcloud beta ai-platform versions create v1 \
  --region=us-east1 \
  --model=getting_started_pytorch \
  --machine-type=n1-standard-4 \
  --image=us-east1-docker.pkg.dev/user-command-nlp/getting-started-pytorch/serve-lstm \
  --ports=8080 \
  --health-route=/ping \
  --predict-route=/predictions/lstm

  curl -d '{"data":"Finished with project data cleaning"}' -H "Authorization: Bearer $(gcloud auth print-access-token)" -H "Content-Type: application/json" -X POST https://us-east1-ml.googleapis.com/v1/projects/user-command-nlp/models/getting_started_pytorch/versions/v1:predict

  torch-model-archiver --force --model-name lstm_project_action --version 1.0 --model-file ./project_action/project_action_lstm.py --serialized-file ./project_action/project_action_trained.pt --export-path model_store --handler ./project_action/project_action_handler.py --requirements-file requirements.txt
  torchserve --start --ncs --model-store model_store --models lstm_project_action.mar --ts-config configuration.properties

  torch-model-archiver --force --model-name lstm_task_action --version 1.0 --model-file ./task_action/task_action_lstm.py --serialized-file ./task_action/task_action_trained.pt --export-path model_store --handler ./task_action/task_action_handler.py --requirements-file requirements.txt
  torchserve --start --ncs --model-store model_store --models lstm_task_action.mar --ts-config configuration.properties

  torch-model-archiver --force --model-name lstm_topic --version 1.0 --model-file ./topic/topic_lstm.py --serialized-file ./topic/topic_trained.pt --export-path model_store --handler ./topic/topic_handler.py --requirements-file requirements.txt
  torchserve --start --ncs --model-store model_store --models lstm_topic.mar --ts-config configuration.properties