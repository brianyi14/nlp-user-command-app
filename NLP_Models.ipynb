{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_data = pd.read_csv('./data/augmented_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes(object):\n",
    "    #initialize model\n",
    "    def __init__(self,data,training_split,use_case,laplace):\n",
    "        self.use_case = use_case\n",
    "        if self.use_case == 'topic':\n",
    "            self.columns = ['Text Command', 'Topic']\n",
    "        else:\n",
    "            self.columns = ['Text Command','Action']\n",
    "            if self.use_case == 'task action':\n",
    "                data = data[data['Topic'] == 'Task']\n",
    "            else:\n",
    "                data = data[data['Topic'] == 'Project']\n",
    "        self.num_rows = len(data)\n",
    "        #shuffle data and reset indexes\n",
    "        self.shuffled_data = data.sample(frac=1).reset_index(drop=True)\n",
    "        #split the data into training and testing sets using input proportion \n",
    "        (self.training_data,self.testing_data) = self.training_testing_split(self.shuffled_data,training_split)\n",
    "        self.num_training_rows,self.num_testing_rows = len(self.training_data),len(self.testing_data)\n",
    "        self.smoothing_constant = laplace\n",
    "        if self.use_case == 'topic':\n",
    "            index_action = {'Task':0, 'Project':1}\n",
    "        if self.use_case == 'task action':\n",
    "            index_action = {'To Do': 0, 'In Progress':1, 'In Review':2, 'Blocked':3, 'Completed':4}\n",
    "        if self.use_case == 'project action':\n",
    "            index_action = {'Create':0, 'On Target':1, 'At Risk':2, 'Danger':3, 'Completed':4}\n",
    "        self.index_action = index_action\n",
    "        self.label_map = dict()\n",
    "        for key in self.index_action:\n",
    "            self.label_map[self.index_action[key]] = key\n",
    "            \n",
    "        \n",
    "    def training_testing_split(self,all_data,training_split):\n",
    "        \n",
    "        breakoff = int(self.num_rows*training_split)\n",
    "        return (all_data.loc[:breakoff],all_data[self.columns].loc[breakoff:])\n",
    "    \n",
    "    #words to ignore when calculating probabilities\n",
    "    def stopwords(self):\n",
    "        return [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
    "\n",
    "    def compute_probabilities(self,text_command_training_data):\n",
    "        action_map = self.label_map\n",
    "        action_probabilities = dict()\n",
    "        action_dict = dict()\n",
    "        word_dict = dict()\n",
    "        word_counter = 0\n",
    "        stopwords = self.stopwords()\n",
    "        for action in list(action_map.values()):\n",
    "            action_dict[action] = dict()\n",
    "        for i in list(text_command_training_data.index.values):\n",
    "            action = text_command_training_data.loc[i][1]\n",
    "            action_probabilities[action] = action_probabilities.get(action,0) + 1\n",
    "            text_command = text_command_training_data.loc[i][0]\n",
    "            for word in text_command.lower().split():\n",
    "                if word not in stopwords:\n",
    "                    word_counter += 1\n",
    "                    word_dict[word] = word_dict.get(word,0) + 1\n",
    "                    action_dict[action][word] = action_dict[action].get(word,0) + 1\n",
    "        num_unique_words = len(word_dict)\n",
    "        for action in action_dict:\n",
    "            num_words = sum(list(action_dict[action].values()))\n",
    "            for word in action_dict[action]:\n",
    "                action_dict[action][word] = (action_dict[action][word]+self.smoothing_constant)/(num_words+self.smoothing_constant*num_unique_words)\n",
    "        for action in action_probabilities:\n",
    "            action_probabilities[action] = action_probabilities[action]/self.num_training_rows\n",
    "        \n",
    "        for word in word_dict:\n",
    "            word_dict[word] = (word_dict[word]+self.smoothing_constant)/(word_counter+self.smoothing_constant*num_unique_words)\n",
    "        self.word_counter = word_counter\n",
    "        return (action_probabilities,action_dict,word_dict)\n",
    "    \n",
    "    def train(self):\n",
    "        text_command_and_action = self.training_data[self.columns]\n",
    "        (self.action_probabilities,self.action_dict,self.word_dict) = self.compute_probabilities(text_command_and_action)\n",
    "    \n",
    "    def predict(self,data):\n",
    "        num_unique_words = len(self.word_dict)\n",
    "        action_map = self.label_map\n",
    "        predictions = pd.DataFrame(columns=[self.columns[0], 'Predicted Action','Predicted Probabilities'])\n",
    "        stopwords = self.stopwords()\n",
    "        for i in list(data.index.values):\n",
    "            words = data.loc[i][0].lower().split()\n",
    "            action_probabilities = []\n",
    "            denominator = 0\n",
    "            for action in list(action_map.values()):\n",
    "                num_words = sum(list(self.action_dict[action].values()))\n",
    "                probability = 1\n",
    "                for word in words:\n",
    "                    if word not in stopwords:\n",
    "                        probability = probability * self.action_dict[action].get(word,(self.smoothing_constant)/(num_words+self.smoothing_constant*num_unique_words))\n",
    "                denominator += probability*self.action_probabilities[action]\n",
    "            for action in list(action_map.values()):\n",
    "                num_words = sum(list(self.action_dict[action].values()))\n",
    "                action_probability = self.action_probabilities[action]\n",
    "                probability = 1\n",
    "                for word in words:\n",
    "                    if word not in stopwords:\n",
    "                        probability = probability * self.action_dict[action].get(word,(self.smoothing_constant)/(num_words+self.smoothing_constant*num_unique_words))\n",
    "                        \n",
    "                probability = (probability * action_probability)/denominator\n",
    "    \n",
    "                \n",
    "                action_probabilities.append(probability)\n",
    "            predictions.loc[i]= [data.loc[i][0],action_map[action_probabilities.index(max(action_probabilities))],action_probabilities]\n",
    "        return predictions\n",
    "    def test(self):\n",
    "        action_map = self.label_map\n",
    "        correct = 0\n",
    "        preds = self.predict(self.testing_data)\n",
    "        actions = self.testing_data[self.columns[1]]\n",
    "        for i in list(self.testing_data.index.values):\n",
    "            if actions.loc[i][0] == preds['Predicted Action'].loc[i][0]:\n",
    "                correct += 1\n",
    "        losses = self.cross_entropy_loss(pd.concat([actions,preds['Predicted Probabilities']],axis=1))\n",
    "        accuracy = correct/self.num_testing_rows\n",
    "        return accuracy, losses, preds\n",
    "        \n",
    "    def softmax(self,labels):\n",
    "        softmax_labels = []\n",
    "        denominator = sum([math.exp(label) for label in labels])\n",
    "        for label in labels:\n",
    "            softmax_labels.append(math.exp(label)/denominator)\n",
    "        return softmax_labels\n",
    "    \n",
    "    def cross_entropy_loss(self,labels):\n",
    "        index_action = self.index_action\n",
    "        losses = dict()\n",
    "        for key in self.label_map:\n",
    "            losses[key] = []\n",
    "        for i in list(labels.index.values):\n",
    "            idx = index_action[labels.loc[i][0]]\n",
    "            loss = - math.log(labels.loc[i][1][idx])\n",
    "            losses[idx].append(loss)\n",
    "        for label in losses:\n",
    "            losses[label] = sum(losses[label])/len(losses[label])\n",
    "        return losses\n",
    "            \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "0.8547258485639686\n",
      "[0.27820476290830426, 0.381128314707332, 0.4989056390600639, 1.0791991981611861, 1.173369574199292]\n"
     ]
    }
   ],
   "source": [
    "topic_accuracies = []\n",
    "cross_entropy_losses = [0,0,0,0,0]\n",
    "for i in range(100):\n",
    "    classifier = NaiveBayes(augmented_data,0.8,'project action',1)\n",
    "    classifier.train()\n",
    "    accuracy, losses, preds = classifier.test()\n",
    "    losses = list(losses.values())\n",
    "    topic_accuracies.append(accuracy)\n",
    "    cross_entropy_losses = [sum(x) for x in zip(cross_entropy_losses,losses)]\n",
    "    if i%10 == 0:\n",
    "        print(i)\n",
    "print(sum(topic_accuracies)/len(topic_accuracies))\n",
    "print([x/len(topic_accuracies) for x in cross_entropy_losses])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
